{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mplhep as hep\n",
    "import matplotlib.pyplot as plt\n",
    "import uproot, os, sys\n",
    "import awkward as ak\n",
    "# Get the notebook directory\n",
    "notebook_dir = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "# Add the project root to sys.path\n",
    "sys.path.append(os.path.join(notebook_dir, \"..\"))\n",
    "from utils.branches import get_branches\n",
    "from utils.plot import plot_data\n",
    "from utils.constants import trigcut, truthpkk\n",
    "from utils.data_loader import load_data\n",
    "from matplotlib import rcParams\n",
    "import matplotlib as mpl\n",
    "plt.style.use(hep.style.LHCb1)\n",
    "config = {\"mathtext.fontset\":'stix'}\n",
    "rcParams.update(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({\n",
    "    # Keep the font family settings for LHCb style\n",
    "    \"font.family\": \"serif\",\n",
    "    \"font.serif\": [\"Times\", \"Computer Modern Roman\", \"DejaVu Serif\"],\n",
    "    \n",
    "    # # Increase only the size-related parameters\n",
    "    # \"figure.figsize\": (15, 10),  # Larger figure\n",
    "    # \"figure.dpi\": 100,          # Screen display\n",
    "    # \"savefig.dpi\": 300,         # Saved figure resolution\n",
    "    \n",
    "    # # # Increase font sizes while keeping LHCb style\n",
    "    \"font.size\": 12,            # Base font size (increase from default)\n",
    "    \"axes.titlesize\": 12,       # Title size\n",
    "    \"axes.labelsize\": 10,       # Axis label size\n",
    "    \"xtick.labelsize\": 12,      # X tick label size\n",
    "    \"ytick.labelsize\": 12,      # Y tick label size\n",
    "    \"legend.fontsize\": 12       # Legend font size\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real Data Files being processed for decay mode L0barPKpKm with tracks ['LL']: ['/share/lazy/Mohamed/Bu2LambdaPPP/RD/restripped.data/reduced/dataBu2L0barPHH_16MD_reduced.root:B2L0barPKpKm_LL/DecayTree', '/share/lazy/Mohamed/Bu2LambdaPPP/RD/restripped.data/reduced/dataBu2L0barPHH_16MU_reduced.root:B2L0barPKpKm_LL/DecayTree', '/share/lazy/Mohamed/Bu2LambdaPPP/RD/restripped.data/reduced/dataBu2L0barPHH_17MD_reduced.root:B2L0barPKpKm_LL/DecayTree', '/share/lazy/Mohamed/Bu2LambdaPPP/RD/restripped.data/reduced/dataBu2L0barPHH_17MU_reduced.root:B2L0barPKpKm_LL/DecayTree', '/share/lazy/Mohamed/Bu2LambdaPPP/RD/restripped.data/reduced/dataBu2L0barPHH_18MD_reduced.root:B2L0barPKpKm_LL/DecayTree', '/share/lazy/Mohamed/Bu2LambdaPPP/RD/restripped.data/reduced/dataBu2L0barPHH_18MU_reduced.root:B2L0barPKpKm_LL/DecayTree']\n",
      "Branches being read: ['h1_P', 'h1_PT', 'h1_PE', 'h1_PX', 'h1_PY', 'h1_PZ', 'h1_ID', 'h1_TRACK_Type', 'h1_IPCHI2_OWNPV', 'h2_P', 'h2_PT', 'h2_PE', 'h2_PX', 'h2_PY', 'h2_PZ', 'h2_ID', 'h2_TRACK_Type', 'h2_IPCHI2_OWNPV', 'p_P', 'p_PT', 'p_PE', 'p_PX', 'p_PY', 'p_PZ', 'p_ID', 'p_TRACK_Type', 'p_IPCHI2_OWNPV', 'h1_MC15TuneV1_ProbNNk', 'h1_MC15TuneV1_ProbNNpi', 'h1_MC15TuneV1_ProbNNp', 'h1_MC15TuneV1_ProbNNmu', 'h2_MC15TuneV1_ProbNNk', 'h2_MC15TuneV1_ProbNNpi', 'h2_MC15TuneV1_ProbNNp', 'h2_MC15TuneV1_ProbNNmu', 'p_MC15TuneV1_ProbNNk', 'p_MC15TuneV1_ProbNNpi', 'p_MC15TuneV1_ProbNNp', 'p_MC15TuneV1_ProbNNmu', 'h1_ProbNNk', 'h1_ProbNNpi', 'h1_ProbNNp', 'h1_ProbNNmu', 'h2_ProbNNk', 'h2_ProbNNpi', 'h2_ProbNNp', 'h2_ProbNNmu', 'p_ProbNNk', 'p_ProbNNpi', 'p_ProbNNp', 'p_ProbNNmu', 'Lp_P', 'Lp_PT', 'Lp_PE', 'Lp_PX', 'Lp_PY', 'Lp_PZ', 'Lp_ID', 'Lp_TRACK_Type', 'Lpi_P', 'Lpi_PT', 'Lpi_PE', 'Lpi_PX', 'Lpi_PY', 'Lpi_PZ', 'Lpi_ID', 'Lpi_TRACK_Type', 'L0_P', 'L0_PT', 'L0_PE', 'L0_PX', 'L0_PY', 'L0_PZ', 'L0_ID', 'L0_MM', 'L0_M', 'Lp_ProbNNp', 'Lpi_ProbNNpi', 'L0_FDCHI2_ORIVX', 'L0_DIRA_OWNPV', 'Lp_MC15TuneV1_ProbNNp', 'Lpi_MC15TuneV1_ProbNNpi', 'Lp_MC15TuneV1_ProbNNpi', 'L0_ENDVERTEX_X', 'L0_ENDVERTEX_Y', 'L0_ENDVERTEX_Z', 'L0_ENDVERTEX_XERR', 'L0_ENDVERTEX_YERR', 'L0_ENDVERTEX_ZERR', 'L0_OWNPV_Z', 'L0_OWNPV_ZERR', 'L0_FD_OWNPV', 'L0_FDCHI2_OWNPV', 'L0_IPCHI2_OWNPV', 'Lp_IPCHI2_OWNPV', 'Lpi_IPCHI2_OWNPV', 'Bu_DTFL0_Lambda0_pplus_PX', 'Bu_DTFL0_Lambda0_pplus_PY', 'Bu_DTFL0_Lambda0_pplus_PZ', 'Bu_DTFL0_Lambda0_pplus_PE', 'Bu_DTFL0_Lambda0_piplus_PX', 'Bu_DTFL0_Lambda0_piplus_PY', 'Bu_DTFL0_Lambda0_piplus_PZ', 'Bu_DTFL0_Lambda0_piplus_PE', 'Bu_DTF_Lambda0_pplus_PX', 'Bu_DTF_Lambda0_pplus_PY', 'Bu_DTF_Lambda0_pplus_PZ', 'Bu_DTF_Lambda0_pplus_PE', 'Bu_DTF_Lambda0_piplus_PX', 'Bu_DTF_Lambda0_piplus_PY', 'Bu_DTF_Lambda0_piplus_PZ', 'Bu_DTF_Lambda0_piplus_PE', 'Bu_FDCHI2_OWNPV', 'nTracks', 'Bu_DTF_Lambda0_decayLength', 'Bu_DTF_Lambda0_decayLengthErr', 'Bu_ENDVERTEX_X', 'Bu_ENDVERTEX_Y', 'Bu_ENDVERTEX_Z', 'Bu_ENDVERTEX_XERR', 'Bu_ENDVERTEX_YERR', 'Bu_ENDVERTEX_ZERR', 'Bu_IPCHI2_OWNPV', 'Bu_MM', 'Bu_MMERR', 'Bu_ID', 'Bu_P', 'Bu_PT', 'Bu_PE', 'Bu_PX', 'Bu_PY', 'Bu_PZ', 'Bu_DTF_nPV', 'Bu_DTF_chi2', 'Bu_DTF_nDOF', 'Bu_DTFL0_chi2', 'Bu_DTFL0_nDOF', 'Bu_DTF_status', 'Bu_DTF_decayLength', 'Bu_DTF_decayLengthErr', 'Bu_DTFL0_ctau', 'Bu_DTFL0_ctauErr', 'Bu_DTF_ctau', 'Bu_DTF_ctauErr', 'Bu_DTFL0_M', 'Bu_DTFL0_MERR', 'Bu_DIRA_OWNPV', 'eventNumber', 'Bu_L0Global_Dec', 'Bu_L0Global_TIS', 'Bu_L0Global_TOS', 'Bu_L0HadronDecision_TIS', 'Bu_L0HadronDecision_TOS', 'Bu_L0MuonDecision_Dec', 'Bu_L0MuonDecision_TIS', 'Bu_L0MuonDecision_TOS', 'Bu_L0MuonHighDecision_Dec', 'Bu_L0MuonHighDecision_TIS', 'Bu_L0MuonHighDecision_TOS', 'Bu_L0DiMuonDecision_Dec', 'Bu_L0DiMuonDecision_TIS', 'Bu_L0DiMuonDecision_TOS', 'Bu_Hlt1TrackMVADecision_Dec', 'Bu_Hlt1TrackMVADecision_TIS', 'Bu_Hlt1TrackMVADecision_TOS', 'Bu_Hlt1TrackMVALooseDecision_Dec', 'Bu_Hlt1TrackMVALooseDecision_TIS', 'Bu_Hlt1TrackMVALooseDecision_TOS', 'Bu_Hlt1TwoTrackMVADecision_Dec', 'Bu_Hlt1TwoTrackMVADecision_TIS', 'Bu_Hlt1TwoTrackMVADecision_TOS', 'Bu_Hlt1MBNoBiasDecision_TOS', 'Bu_Hlt2Topo2BodyDecision_Dec', 'Bu_Hlt2Topo2BodyDecision_TIS', 'Bu_Hlt2Topo2BodyDecision_TOS', 'Bu_Hlt2Topo3BodyDecision_Dec', 'Bu_Hlt2Topo3BodyDecision_TIS', 'Bu_Hlt2Topo3BodyDecision_TOS', 'Bu_Hlt2Topo4BodyDecision_Dec', 'Bu_Hlt2Topo4BodyDecision_TIS', 'Bu_Hlt2Topo4BodyDecision_TOS']\n"
     ]
    }
   ],
   "source": [
    "# data_path = \"/share/lazy/Mohamed/Bu2LambdaPPP/RD/restripped.data/reduced\"\n",
    "# decay_modes = [\"L0barPKpKm\"]\n",
    "# particles = [\"h1\", \"h2\", \"p\"]\n",
    "\n",
    "# data_ll = load_data(\n",
    "#     data_path=data_path,\n",
    "#     decay_modes=decay_modes,\n",
    "#     tracks=[\"LL\"],\n",
    "#     particles=particles\n",
    "# )\n",
    "\n",
    "# data_dd = load_data(\n",
    "#     data_path=data_path,\n",
    "#     decay_modes=decay_modes,\n",
    "#     tracks=[\"DD\"],\n",
    "#     particles=particles\n",
    "# )\n",
    "\n",
    "data_path = \"/share/lazy/Mohamed/Bu2LambdaPPP/RD/restripped.data/reduced\"\n",
    "particles = [\"h1\", \"h2\", \"p\"]\n",
    "data_ll = load_data(\n",
    "    data_path=data_path,\n",
    "    decay_mode=\"L0barPKpKm\",  # or \"L0PbarKpKp\"\n",
    "    tracks=[\"LL\"],\n",
    "    particles=particles\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real Data Files being processed for decay mode L0barPKpKm with tracks ['LL']: ['/share/lazy/Mohamed/Bu2LambdaPPP/RD/restripped.data/reduced/dataBu2L0barPHH_16MD_reduced.root:B2L0barPKpKm_LL/DecayTree', '/share/lazy/Mohamed/Bu2LambdaPPP/RD/restripped.data/reduced/dataBu2L0barPHH_16MU_reduced.root:B2L0barPKpKm_LL/DecayTree', '/share/lazy/Mohamed/Bu2LambdaPPP/RD/restripped.data/reduced/dataBu2L0barPHH_17MD_reduced.root:B2L0barPKpKm_LL/DecayTree', '/share/lazy/Mohamed/Bu2LambdaPPP/RD/restripped.data/reduced/dataBu2L0barPHH_17MU_reduced.root:B2L0barPKpKm_LL/DecayTree', '/share/lazy/Mohamed/Bu2LambdaPPP/RD/restripped.data/reduced/dataBu2L0barPHH_18MD_reduced.root:B2L0barPKpKm_LL/DecayTree', '/share/lazy/Mohamed/Bu2LambdaPPP/RD/restripped.data/reduced/dataBu2L0barPHH_18MU_reduced.root:B2L0barPKpKm_LL/DecayTree']\n",
      "Branches being read: ['h1_P', 'h1_PT', 'h1_PE', 'h1_PX', 'h1_PY', 'h1_PZ', 'h1_ID', 'h1_TRACK_Type', 'h1_IPCHI2_OWNPV', 'h2_P', 'h2_PT', 'h2_PE', 'h2_PX', 'h2_PY', 'h2_PZ', 'h2_ID', 'h2_TRACK_Type', 'h2_IPCHI2_OWNPV', 'p_P', 'p_PT', 'p_PE', 'p_PX', 'p_PY', 'p_PZ', 'p_ID', 'p_TRACK_Type', 'p_IPCHI2_OWNPV', 'h1_MC15TuneV1_ProbNNk', 'h1_MC15TuneV1_ProbNNpi', 'h1_MC15TuneV1_ProbNNp', 'h1_MC15TuneV1_ProbNNmu', 'h2_MC15TuneV1_ProbNNk', 'h2_MC15TuneV1_ProbNNpi', 'h2_MC15TuneV1_ProbNNp', 'h2_MC15TuneV1_ProbNNmu', 'p_MC15TuneV1_ProbNNk', 'p_MC15TuneV1_ProbNNpi', 'p_MC15TuneV1_ProbNNp', 'p_MC15TuneV1_ProbNNmu', 'h1_ProbNNk', 'h1_ProbNNpi', 'h1_ProbNNp', 'h1_ProbNNmu', 'h2_ProbNNk', 'h2_ProbNNpi', 'h2_ProbNNp', 'h2_ProbNNmu', 'p_ProbNNk', 'p_ProbNNpi', 'p_ProbNNp', 'p_ProbNNmu', 'Lp_P', 'Lp_PT', 'Lp_PE', 'Lp_PX', 'Lp_PY', 'Lp_PZ', 'Lp_ID', 'Lp_TRACK_Type', 'Lpi_P', 'Lpi_PT', 'Lpi_PE', 'Lpi_PX', 'Lpi_PY', 'Lpi_PZ', 'Lpi_ID', 'Lpi_TRACK_Type', 'L0_P', 'L0_PT', 'L0_PE', 'L0_PX', 'L0_PY', 'L0_PZ', 'L0_ID', 'L0_MM', 'L0_M', 'Lp_ProbNNp', 'Lpi_ProbNNpi', 'L0_FDCHI2_ORIVX', 'L0_DIRA_OWNPV', 'Lp_MC15TuneV1_ProbNNp', 'Lpi_MC15TuneV1_ProbNNpi', 'Lp_MC15TuneV1_ProbNNpi', 'L0_ENDVERTEX_X', 'L0_ENDVERTEX_Y', 'L0_ENDVERTEX_Z', 'L0_ENDVERTEX_XERR', 'L0_ENDVERTEX_YERR', 'L0_ENDVERTEX_ZERR', 'L0_OWNPV_Z', 'L0_OWNPV_ZERR', 'L0_FD_OWNPV', 'L0_FDCHI2_OWNPV', 'L0_IPCHI2_OWNPV', 'Lp_IPCHI2_OWNPV', 'Lpi_IPCHI2_OWNPV', 'Bu_DTFL0_Lambda0_pplus_PX', 'Bu_DTFL0_Lambda0_pplus_PY', 'Bu_DTFL0_Lambda0_pplus_PZ', 'Bu_DTFL0_Lambda0_pplus_PE', 'Bu_DTFL0_Lambda0_piplus_PX', 'Bu_DTFL0_Lambda0_piplus_PY', 'Bu_DTFL0_Lambda0_piplus_PZ', 'Bu_DTFL0_Lambda0_piplus_PE', 'Bu_DTF_Lambda0_pplus_PX', 'Bu_DTF_Lambda0_pplus_PY', 'Bu_DTF_Lambda0_pplus_PZ', 'Bu_DTF_Lambda0_pplus_PE', 'Bu_DTF_Lambda0_piplus_PX', 'Bu_DTF_Lambda0_piplus_PY', 'Bu_DTF_Lambda0_piplus_PZ', 'Bu_DTF_Lambda0_piplus_PE', 'Bu_FDCHI2_OWNPV', 'nTracks', 'Bu_DTF_Lambda0_decayLength', 'Bu_DTF_Lambda0_decayLengthErr', 'Bu_ENDVERTEX_X', 'Bu_ENDVERTEX_Y', 'Bu_ENDVERTEX_Z', 'Bu_ENDVERTEX_XERR', 'Bu_ENDVERTEX_YERR', 'Bu_ENDVERTEX_ZERR', 'Bu_IPCHI2_OWNPV', 'Bu_MM', 'Bu_MMERR', 'Bu_ID', 'Bu_P', 'Bu_PT', 'Bu_PE', 'Bu_PX', 'Bu_PY', 'Bu_PZ', 'Bu_DTF_nPV', 'Bu_DTF_chi2', 'Bu_DTF_nDOF', 'Bu_DTFL0_chi2', 'Bu_DTFL0_nDOF', 'Bu_DTF_status', 'Bu_DTF_decayLength', 'Bu_DTF_decayLengthErr', 'Bu_DTFL0_ctau', 'Bu_DTFL0_ctauErr', 'Bu_DTF_ctau', 'Bu_DTF_ctauErr', 'Bu_DTFL0_M', 'Bu_DTFL0_MERR', 'Bu_DIRA_OWNPV', 'eventNumber', 'Bu_L0Global_Dec', 'Bu_L0Global_TIS', 'Bu_L0Global_TOS', 'Bu_L0HadronDecision_TIS', 'Bu_L0HadronDecision_TOS', 'Bu_L0MuonDecision_Dec', 'Bu_L0MuonDecision_TIS', 'Bu_L0MuonDecision_TOS', 'Bu_L0MuonHighDecision_Dec', 'Bu_L0MuonHighDecision_TIS', 'Bu_L0MuonHighDecision_TOS', 'Bu_L0DiMuonDecision_Dec', 'Bu_L0DiMuonDecision_TIS', 'Bu_L0DiMuonDecision_TOS', 'Bu_Hlt1TrackMVADecision_Dec', 'Bu_Hlt1TrackMVADecision_TIS', 'Bu_Hlt1TrackMVADecision_TOS', 'Bu_Hlt1TrackMVALooseDecision_Dec', 'Bu_Hlt1TrackMVALooseDecision_TIS', 'Bu_Hlt1TrackMVALooseDecision_TOS', 'Bu_Hlt1TwoTrackMVADecision_Dec', 'Bu_Hlt1TwoTrackMVADecision_TIS', 'Bu_Hlt1TwoTrackMVADecision_TOS', 'Bu_Hlt1MBNoBiasDecision_TOS', 'Bu_Hlt2Topo2BodyDecision_Dec', 'Bu_Hlt2Topo2BodyDecision_TIS', 'Bu_Hlt2Topo2BodyDecision_TOS', 'Bu_Hlt2Topo3BodyDecision_Dec', 'Bu_Hlt2Topo3BodyDecision_TIS', 'Bu_Hlt2Topo3BodyDecision_TOS', 'Bu_Hlt2Topo4BodyDecision_Dec', 'Bu_Hlt2Topo4BodyDecision_TIS', 'Bu_Hlt2Topo4BodyDecision_TOS']\n",
      "Real Data Files being processed for decay mode L0barPKpKm with tracks ['DD']: ['/share/lazy/Mohamed/Bu2LambdaPPP/RD/restripped.data/reduced/dataBu2L0barPHH_16MD_reduced.root:B2L0barPKpKm_DD/DecayTree', '/share/lazy/Mohamed/Bu2LambdaPPP/RD/restripped.data/reduced/dataBu2L0barPHH_16MU_reduced.root:B2L0barPKpKm_DD/DecayTree', '/share/lazy/Mohamed/Bu2LambdaPPP/RD/restripped.data/reduced/dataBu2L0barPHH_17MD_reduced.root:B2L0barPKpKm_DD/DecayTree', '/share/lazy/Mohamed/Bu2LambdaPPP/RD/restripped.data/reduced/dataBu2L0barPHH_17MU_reduced.root:B2L0barPKpKm_DD/DecayTree', '/share/lazy/Mohamed/Bu2LambdaPPP/RD/restripped.data/reduced/dataBu2L0barPHH_18MD_reduced.root:B2L0barPKpKm_DD/DecayTree', '/share/lazy/Mohamed/Bu2LambdaPPP/RD/restripped.data/reduced/dataBu2L0barPHH_18MU_reduced.root:B2L0barPKpKm_DD/DecayTree']\n",
      "Branches being read: ['h1_P', 'h1_PT', 'h1_PE', 'h1_PX', 'h1_PY', 'h1_PZ', 'h1_ID', 'h1_TRACK_Type', 'h1_IPCHI2_OWNPV', 'h2_P', 'h2_PT', 'h2_PE', 'h2_PX', 'h2_PY', 'h2_PZ', 'h2_ID', 'h2_TRACK_Type', 'h2_IPCHI2_OWNPV', 'p_P', 'p_PT', 'p_PE', 'p_PX', 'p_PY', 'p_PZ', 'p_ID', 'p_TRACK_Type', 'p_IPCHI2_OWNPV', 'h1_MC15TuneV1_ProbNNk', 'h1_MC15TuneV1_ProbNNpi', 'h1_MC15TuneV1_ProbNNp', 'h1_MC15TuneV1_ProbNNmu', 'h2_MC15TuneV1_ProbNNk', 'h2_MC15TuneV1_ProbNNpi', 'h2_MC15TuneV1_ProbNNp', 'h2_MC15TuneV1_ProbNNmu', 'p_MC15TuneV1_ProbNNk', 'p_MC15TuneV1_ProbNNpi', 'p_MC15TuneV1_ProbNNp', 'p_MC15TuneV1_ProbNNmu', 'h1_ProbNNk', 'h1_ProbNNpi', 'h1_ProbNNp', 'h1_ProbNNmu', 'h2_ProbNNk', 'h2_ProbNNpi', 'h2_ProbNNp', 'h2_ProbNNmu', 'p_ProbNNk', 'p_ProbNNpi', 'p_ProbNNp', 'p_ProbNNmu', 'Lp_P', 'Lp_PT', 'Lp_PE', 'Lp_PX', 'Lp_PY', 'Lp_PZ', 'Lp_ID', 'Lp_TRACK_Type', 'Lpi_P', 'Lpi_PT', 'Lpi_PE', 'Lpi_PX', 'Lpi_PY', 'Lpi_PZ', 'Lpi_ID', 'Lpi_TRACK_Type', 'L0_P', 'L0_PT', 'L0_PE', 'L0_PX', 'L0_PY', 'L0_PZ', 'L0_ID', 'L0_MM', 'L0_M', 'Lp_ProbNNp', 'Lpi_ProbNNpi', 'L0_FDCHI2_ORIVX', 'L0_DIRA_OWNPV', 'Lp_MC15TuneV1_ProbNNp', 'Lpi_MC15TuneV1_ProbNNpi', 'Lp_MC15TuneV1_ProbNNpi', 'L0_ENDVERTEX_X', 'L0_ENDVERTEX_Y', 'L0_ENDVERTEX_Z', 'L0_ENDVERTEX_XERR', 'L0_ENDVERTEX_YERR', 'L0_ENDVERTEX_ZERR', 'L0_OWNPV_Z', 'L0_OWNPV_ZERR', 'L0_FD_OWNPV', 'L0_FDCHI2_OWNPV', 'L0_IPCHI2_OWNPV', 'Lp_IPCHI2_OWNPV', 'Lpi_IPCHI2_OWNPV', 'Bu_DTFL0_Lambda0_pplus_PX', 'Bu_DTFL0_Lambda0_pplus_PY', 'Bu_DTFL0_Lambda0_pplus_PZ', 'Bu_DTFL0_Lambda0_pplus_PE', 'Bu_DTFL0_Lambda0_piplus_PX', 'Bu_DTFL0_Lambda0_piplus_PY', 'Bu_DTFL0_Lambda0_piplus_PZ', 'Bu_DTFL0_Lambda0_piplus_PE', 'Bu_DTF_Lambda0_pplus_PX', 'Bu_DTF_Lambda0_pplus_PY', 'Bu_DTF_Lambda0_pplus_PZ', 'Bu_DTF_Lambda0_pplus_PE', 'Bu_DTF_Lambda0_piplus_PX', 'Bu_DTF_Lambda0_piplus_PY', 'Bu_DTF_Lambda0_piplus_PZ', 'Bu_DTF_Lambda0_piplus_PE', 'Bu_FDCHI2_OWNPV', 'nTracks', 'Bu_DTF_Lambda0_decayLength', 'Bu_DTF_Lambda0_decayLengthErr', 'Bu_ENDVERTEX_X', 'Bu_ENDVERTEX_Y', 'Bu_ENDVERTEX_Z', 'Bu_ENDVERTEX_XERR', 'Bu_ENDVERTEX_YERR', 'Bu_ENDVERTEX_ZERR', 'Bu_IPCHI2_OWNPV', 'Bu_MM', 'Bu_MMERR', 'Bu_ID', 'Bu_P', 'Bu_PT', 'Bu_PE', 'Bu_PX', 'Bu_PY', 'Bu_PZ', 'Bu_DTF_nPV', 'Bu_DTF_chi2', 'Bu_DTF_nDOF', 'Bu_DTFL0_chi2', 'Bu_DTFL0_nDOF', 'Bu_DTF_status', 'Bu_DTF_decayLength', 'Bu_DTF_decayLengthErr', 'Bu_DTFL0_ctau', 'Bu_DTFL0_ctauErr', 'Bu_DTF_ctau', 'Bu_DTF_ctauErr', 'Bu_DTFL0_M', 'Bu_DTFL0_MERR', 'Bu_DIRA_OWNPV', 'eventNumber', 'Bu_L0Global_Dec', 'Bu_L0Global_TIS', 'Bu_L0Global_TOS', 'Bu_L0HadronDecision_TIS', 'Bu_L0HadronDecision_TOS', 'Bu_L0MuonDecision_Dec', 'Bu_L0MuonDecision_TIS', 'Bu_L0MuonDecision_TOS', 'Bu_L0MuonHighDecision_Dec', 'Bu_L0MuonHighDecision_TIS', 'Bu_L0MuonHighDecision_TOS', 'Bu_L0DiMuonDecision_Dec', 'Bu_L0DiMuonDecision_TIS', 'Bu_L0DiMuonDecision_TOS', 'Bu_Hlt1TrackMVADecision_Dec', 'Bu_Hlt1TrackMVADecision_TIS', 'Bu_Hlt1TrackMVADecision_TOS', 'Bu_Hlt1TrackMVALooseDecision_Dec', 'Bu_Hlt1TrackMVALooseDecision_TIS', 'Bu_Hlt1TrackMVALooseDecision_TOS', 'Bu_Hlt1TwoTrackMVADecision_Dec', 'Bu_Hlt1TwoTrackMVADecision_TIS', 'Bu_Hlt1TwoTrackMVADecision_TOS', 'Bu_Hlt1MBNoBiasDecision_TOS', 'Bu_Hlt2Topo2BodyDecision_Dec', 'Bu_Hlt2Topo2BodyDecision_TIS', 'Bu_Hlt2Topo2BodyDecision_TOS', 'Bu_Hlt2Topo3BodyDecision_Dec', 'Bu_Hlt2Topo3BodyDecision_TIS', 'Bu_Hlt2Topo3BodyDecision_TOS', 'Bu_Hlt2Topo4BodyDecision_Dec', 'Bu_Hlt2Topo4BodyDecision_TIS', 'Bu_Hlt2Topo4BodyDecision_TOS']\n"
     ]
    }
   ],
   "source": [
    "data_path = \"/share/lazy/Mohamed/Bu2LambdaPPP/RD/restripped.data/reduced\"\n",
    "decay_modes = \"L0barPKpKm\"\n",
    "particles = [\"h1\", \"h2\", \"p\"]\n",
    "\n",
    "data_ll = load_data(\n",
    "    data_path=data_path,\n",
    "    decay_mode=decay_modes,\n",
    "    tracks=[\"LL\"],\n",
    "    particles=particles\n",
    ")\n",
    "\n",
    "data_dd = load_data(\n",
    "    data_path=data_path,\n",
    "    decay_mode=decay_modes,\n",
    "    tracks=[\"DD\"],\n",
    "    particles=particles\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Apply the cuts to get the selection masks\\nll_mask, dd_mask, ll_summary, dd_summary, ll_kk_product, dd_kk_product = apply_cuts_to_samples(mc_ll, mc_dd)\\n\\n# Apply the masks to get the selected events\\nmc_ll_selected = apply_mask_to_data(mc_ll, ll_mask)\\nmc_dd_selected = apply_mask_to_data(mc_dd, dd_mask)\\n\\n# Now you can proceed with analysis using the selected samples and KK product information\\n# Further operations with mc_ll_selected, mc_dd_selected, ll_kk_product, dd_kk_product...\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import awkward as ak\n",
    "from collections import OrderedDict\n",
    "\n",
    "def apply_selection_cuts(events, track_type='LL'):\n",
    "    \"\"\"\n",
    "    Apply selection cuts to B+ → Λ0 h1 h2 samples based on specific criteria\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    events : awkward.Array or dict-like object\n",
    "        Events from uproot containing the MC sample\n",
    "    track_type : str\n",
    "        Track type, either 'LL' (Long-Long) or 'DD' (Downstream-Downstream)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    numpy.ndarray\n",
    "        Boolean mask of selected events\n",
    "    dict\n",
    "        Summary of the cuts applied\n",
    "    \"\"\"\n",
    "    # Initialize mask with all True\n",
    "    mask = np.ones(len(events), dtype=bool)\n",
    "    \n",
    "    # Track the cuts for debugging and reporting\n",
    "    cuts_summary = OrderedDict()\n",
    "    initial_events = len(events)\n",
    "    \n",
    "    # ===== p (Proton) Cuts =====\n",
    "    # MC15TuneV1_ProbNNp > 0.05\n",
    "    p_prob_cut = events['p_MC15TuneV1_ProbNNp'] > 0.05\n",
    "    mask = mask & p_prob_cut\n",
    "    cuts_summary['proton_prob_cut'] = np.sum(p_prob_cut)\n",
    "    \n",
    "    # ===== Λ0 Cuts =====\n",
    "    \n",
    "    # ΔZ > 20 mm (difference between Lambda decay vertex and primary vertex)\n",
    "    delta_z = events['L0_ENDVERTEX_Z'] - events['L0_OWNPV_Z']\n",
    "    delta_z_cut = delta_z > 20\n",
    "    mask = mask & delta_z_cut\n",
    "    cuts_summary['delta_z_cut'] = np.sum(delta_z_cut)\n",
    "    \n",
    "    # χ²FD > 45 (Lambda flight distance chi2)\n",
    "    fd_chi2_cut = events['L0_FDCHI2_OWNPV'] > 45\n",
    "    mask = mask & fd_chi2_cut\n",
    "    cuts_summary['fd_chi2_cut'] = np.sum(fd_chi2_cut)\n",
    "    \n",
    "    # |m(pπ⁻) - 1115.6| < 6 MeV/c²\n",
    "    lambda_mass_diff = np.abs(events['L0_M'] - 1115.6)\n",
    "    lambda_mass_cut = lambda_mass_diff < 6\n",
    "    mask = mask & lambda_mass_cut\n",
    "    cuts_summary['lambda_mass_cut'] = np.sum(lambda_mass_cut)\n",
    "    \n",
    "    # Lp_MC15TuneV1_ProbNNp > 0.2\n",
    "    lp_prob_cut = events['Lp_MC15TuneV1_ProbNNp'] > 0.2\n",
    "    mask = mask & lp_prob_cut\n",
    "    cuts_summary['lp_prob_cut'] = np.sum(lp_prob_cut)\n",
    "    \n",
    "    # ===== h1 and h2 (Kaon) Cuts =====\n",
    "    \n",
    "    # Calculate KK product = h1_ProbNNk × h2_ProbNNk\n",
    "    kk_product = events['h1_ProbNNk'] * events['h2_ProbNNk']\n",
    "    \n",
    "    # Apply a threshold cut on the KK product if needed\n",
    "    # Threshold value can be adjusted later as required\n",
    "    kk_product_threshold = 0.04  # Example threshold (0.2 * 0.2)\n",
    "    kk_product_cut = kk_product > kk_product_threshold\n",
    "    mask = mask & kk_product_cut\n",
    "    cuts_summary['kk_product_cut'] = np.sum(kk_product_cut)\n",
    "    \n",
    "    # Add individual kaon cuts for reference\n",
    "    h1_kaon_cut = events['h1_ProbNNk'] > 0.2\n",
    "    cuts_summary['h1_kaon_cut'] = np.sum(h1_kaon_cut)\n",
    "    \n",
    "    h2_kaon_cut = events['h2_ProbNNk'] > 0.2\n",
    "    cuts_summary['h2_kaon_cut'] = np.sum(h2_kaon_cut)\n",
    "    \n",
    "    # ===== B⁺ Cuts =====\n",
    "    \n",
    "    # pT > 3000 MeV/c\n",
    "    b_pt_cut = events['Bu_PT'] > 3000\n",
    "    mask = mask & b_pt_cut\n",
    "    cuts_summary['b_pt_cut'] = np.sum(b_pt_cut)\n",
    "    \n",
    "    # χ²DTF < 30 & Converged (DTF = Decay Tree Fitter)\n",
    "    dtf_chi2 = events['Bu_DTF_chi2']\n",
    "    dtf_chi2_cut = (dtf_chi2 < 30) \n",
    "    mask = mask & dtf_chi2_cut\n",
    "    cuts_summary['dtf_chi2_cut'] = np.sum(dtf_chi2_cut)\n",
    "    \n",
    "    # χ²IP < 10 (Impact Parameter Chi2)\n",
    "    ip_chi2_cut = events['Bu_IPCHI2_OWNPV'] < 10\n",
    "    mask = mask & ip_chi2_cut\n",
    "    cuts_summary['ip_chi2_cut'] = np.sum(ip_chi2_cut)\n",
    "    \n",
    "    # χ²FD > 175 (Flight Distance Chi2)\n",
    "    b_fd_chi2_cut = events['Bu_FDCHI2_OWNPV'] > 175\n",
    "    mask = mask & b_fd_chi2_cut\n",
    "    cuts_summary['b_fd_chi2_cut'] = np.sum(b_fd_chi2_cut)\n",
    "    \n",
    "    # Print selection summary\n",
    "    selected_events = np.sum(mask)\n",
    "    print(f\"Selection summary for {track_type} sample:\")\n",
    "    print(f\"Initial events: {initial_events}\")\n",
    "    for cut_name, cut_count in cuts_summary.items():\n",
    "        print(f\"  {cut_name}: {cut_count} / {initial_events} ({cut_count/initial_events:.2%})\")\n",
    "    print(f\"Final selected events: {selected_events} / {initial_events} ({selected_events/initial_events:.2%})\")\n",
    "    \n",
    "    return mask, cuts_summary, kk_product  # Return kk_product for further analysis if needed\n",
    "\n",
    "def apply_cuts_to_samples(mc_ll, mc_dd):\n",
    "    \"\"\"\n",
    "    Apply selection cuts to both LL and DD samples\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    mc_ll : awkward.Array or dict-like object\n",
    "        Long-Long track type MC sample\n",
    "    mc_dd : awkward.Array or dict-like object\n",
    "        Downstream-Downstream track type MC sample\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        (mc_ll_mask, mc_dd_mask, ll_cuts_summary, dd_cuts_summary, ll_kk_product, dd_kk_product)\n",
    "    \"\"\"\n",
    "    # Apply cuts to LL sample\n",
    "    ll_mask, ll_cuts_summary, ll_kk_product = apply_selection_cuts(mc_ll, track_type='LL')\n",
    "    \n",
    "    # Apply cuts to DD sample\n",
    "    dd_mask, dd_cuts_summary, dd_kk_product = apply_selection_cuts(mc_dd, track_type='DD')\n",
    "    \n",
    "    # Print comparison between LL and DD\n",
    "    ll_total = len(mc_ll)\n",
    "    dd_total = len(mc_dd)\n",
    "    ll_selected = np.sum(ll_mask)\n",
    "    dd_selected = np.sum(dd_mask)\n",
    "    \n",
    "    print(\"\\nComparison between LL and DD selection efficiency:\")\n",
    "    print(f\"LL: {ll_selected}/{ll_total} ({ll_selected/ll_total:.2%})\")\n",
    "    print(f\"DD: {dd_selected}/{dd_total} ({dd_selected/dd_total:.2%})\")\n",
    "    \n",
    "    # Print KK product statistics\n",
    "    print(\"\\nKK Product Statistics:\")\n",
    "    print(f\"LL - Mean: {np.mean(ll_kk_product):.4f}, Median: {np.median(ll_kk_product):.4f}\")\n",
    "    print(f\"DD - Mean: {np.mean(dd_kk_product):.4f}, Median: {np.median(dd_kk_product):.4f}\")\n",
    "    \n",
    "    return ll_mask, dd_mask, ll_cuts_summary, dd_cuts_summary, ll_kk_product, dd_kk_product\n",
    "\n",
    "def apply_mask_to_data(events, mask):\n",
    "    \"\"\"\n",
    "    Apply a boolean mask to event data\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    events : awkward.Array or dict-like object\n",
    "        Event data\n",
    "    mask : numpy.ndarray\n",
    "        Boolean mask to apply\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    awkward.Array or dict-like object\n",
    "        Selected events\n",
    "    \"\"\"\n",
    "    if hasattr(events, 'mask'):\n",
    "        # For awkward arrays\n",
    "        return events[mask]\n",
    "    else:\n",
    "        # For dictionary-like objects\n",
    "        selected = {}\n",
    "        for key, array in events.items():\n",
    "            selected[key] = array[mask]\n",
    "        return selected\n",
    "\n",
    "# Usage example:\n",
    "\"\"\"\n",
    "# Apply the cuts to get the selection masks\n",
    "ll_mask, dd_mask, ll_summary, dd_summary, ll_kk_product, dd_kk_product = apply_cuts_to_samples(mc_ll, mc_dd)\n",
    "\n",
    "# Apply the masks to get the selected events\n",
    "mc_ll_selected = apply_mask_to_data(mc_ll, ll_mask)\n",
    "mc_dd_selected = apply_mask_to_data(mc_dd, dd_mask)\n",
    "\n",
    "# Now you can proceed with analysis using the selected samples and KK product information\n",
    "# Further operations with mc_ll_selected, mc_dd_selected, ll_kk_product, dd_kk_product...\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selection summary for LL sample:\n",
      "Initial events: 713632\n",
      "  proton_prob_cut: 713632 / 713632 (100.00%)\n",
      "  delta_z_cut: 629470 / 713632 (88.21%)\n",
      "  fd_chi2_cut: 643705 / 713632 (90.20%)\n",
      "  lambda_mass_cut: 378105 / 713632 (52.98%)\n",
      "  lp_prob_cut: 491539 / 713632 (68.88%)\n",
      "  kk_product_cut: 713632 / 713632 (100.00%)\n",
      "  h1_kaon_cut: 596584 / 713632 (83.60%)\n",
      "  h2_kaon_cut: 596142 / 713632 (83.54%)\n",
      "  b_pt_cut: 713632 / 713632 (100.00%)\n",
      "  dtf_chi2_cut: 713632 / 713632 (100.00%)\n",
      "  ip_chi2_cut: 713632 / 713632 (100.00%)\n",
      "  b_fd_chi2_cut: 713632 / 713632 (100.00%)\n",
      "Final selected events: 262305 / 713632 (36.76%)\n",
      "Selection summary for DD sample:\n",
      "Initial events: 561928\n",
      "  proton_prob_cut: 561928 / 561928 (100.00%)\n",
      "  delta_z_cut: 561873 / 561928 (99.99%)\n",
      "  fd_chi2_cut: 529917 / 561928 (94.30%)\n",
      "  lambda_mass_cut: 488844 / 561928 (86.99%)\n",
      "  lp_prob_cut: 517923 / 561928 (92.17%)\n",
      "  kk_product_cut: 561928 / 561928 (100.00%)\n",
      "  h1_kaon_cut: 462603 / 561928 (82.32%)\n",
      "  h2_kaon_cut: 475274 / 561928 (84.58%)\n",
      "  b_pt_cut: 561928 / 561928 (100.00%)\n",
      "  dtf_chi2_cut: 561928 / 561928 (100.00%)\n",
      "  ip_chi2_cut: 561928 / 561928 (100.00%)\n",
      "  b_fd_chi2_cut: 561928 / 561928 (100.00%)\n",
      "Final selected events: 437936 / 561928 (77.93%)\n",
      "\n",
      "Comparison between LL and DD selection efficiency:\n",
      "LL: 262305/713632 (36.76%)\n",
      "DD: 437936/561928 (77.93%)\n",
      "\n",
      "KK Product Statistics:\n",
      "LL - Mean: 0.3349, Median: 0.1936\n",
      "DD - Mean: 0.4212, Median: 0.2818\n"
     ]
    }
   ],
   "source": [
    "selected_ll, selected_dd, ll_summary, dd_summary, ll_kk_product, dd_kk_product = apply_cuts_to_samples(data_ll, data_dd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import awkward as ak\n",
    "\n",
    "def plot_selection_variables_separate(mc_ll, mc_dd, output_prefix=\"selection_variables\"):\n",
    "    \"\"\"\n",
    "    Plot selection variables for LL and DD samples in separate files with 6 subplots per page\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    mc_ll : awkward.Array\n",
    "        Long-Long track type MC sample\n",
    "    mc_dd : awkward.Array\n",
    "        Downstream-Downstream track type MC sample\n",
    "    output_prefix : str\n",
    "        Prefix for output files\n",
    "    \"\"\"\n",
    "    # Define variables to plot and their properties\n",
    "    variables = [\n",
    "        {\n",
    "            'name': 'p_MC15TuneV1_ProbNNp',\n",
    "            'label': 'p_MC15TuneV1_ProbNNp',\n",
    "            'cut_value': 0.05,\n",
    "            'cut_type': '>'\n",
    "        },\n",
    "        {\n",
    "            'name': 'L0_FDCHI2_OWNPV',\n",
    "            'label': 'Lambda χ²FD',\n",
    "            'cut_value': 45,\n",
    "            'cut_type': '>'\n",
    "        },\n",
    "        {\n",
    "            'name': 'Lp_MC15TuneV1_ProbNNp',\n",
    "            'label': 'Lp_MC15TuneV1_ProbNNp',\n",
    "            'cut_value': 0.2,\n",
    "            'cut_type': '>'\n",
    "        },\n",
    "        {\n",
    "            'name': 'Bu_PT',\n",
    "            'label': 'B+ pT [MeV/c]',\n",
    "            'cut_value': 3000,\n",
    "            'cut_type': '>'\n",
    "        },\n",
    "        {\n",
    "            'name': 'Bu_IPCHI2_OWNPV',\n",
    "            'label': 'B+ χ²IP',\n",
    "            'cut_value': 10,\n",
    "            'cut_type': '<'\n",
    "        },\n",
    "        {\n",
    "            'name': 'Bu_FDCHI2_OWNPV',\n",
    "            'label': 'B+ χ²FD',\n",
    "            'cut_value': 175,\n",
    "            'cut_type': '>'\n",
    "        },\n",
    "        {\n",
    "            'name': 'h1_ProbNNk',\n",
    "            'label': 'h1_ProbNNk',\n",
    "            'cut_value': 0.2,\n",
    "            'cut_type': '>'\n",
    "        },\n",
    "        {\n",
    "            'name': 'h2_ProbNNk',\n",
    "            'label': 'h2_ProbNNk',\n",
    "            'cut_value': 0.2,\n",
    "            'cut_type': '>'\n",
    "        },\n",
    "        # We'll handle Bu_DTF_chi2 separately\n",
    "    ]\n",
    "    \n",
    "    # Add delta_z calculation separately\n",
    "    delta_z = {\n",
    "        'name': 'delta_z',\n",
    "        'label': 'ΔZ [mm]',\n",
    "        'cut_value': 20,\n",
    "        'cut_type': '>'\n",
    "    }\n",
    "    \n",
    "    # Add Bu_DTF_chi2 separately with special handling\n",
    "    bu_dtf_chi2 = {\n",
    "        'name': 'Bu_DTF_chi2',\n",
    "        'label': 'B+ χ²DTF',\n",
    "        'cut_value': 30,\n",
    "        'cut_type': '<'\n",
    "    }\n",
    "    \n",
    "    # Add KK product separately\n",
    "    kk_product = {\n",
    "        'name': 'kk_product',\n",
    "        'label': 'h1_ProbNNk × h2_ProbNNk',\n",
    "        'cut_value': 0.2,\n",
    "        'cut_type': '>'\n",
    "    }\n",
    "    \n",
    "    # Total number of variables\n",
    "    all_vars = [delta_z] + variables + [bu_dtf_chi2, kk_product]\n",
    "    n_vars = len(all_vars)\n",
    "    \n",
    "    # Calculate how many plots we need with 6 subplots per page\n",
    "    vars_per_page = 6\n",
    "    n_pages = (n_vars + vars_per_page - 1) // vars_per_page  # Ceiling division\n",
    "    \n",
    "    # Define grid layout for each page (3x2 grid)\n",
    "    n_rows, n_cols = 2, 3\n",
    "    \n",
    "    # Create separate figures for each page of LL and DD\n",
    "    ll_figs = []\n",
    "    ll_axes = []\n",
    "    dd_figs = []\n",
    "    dd_axes = []\n",
    "    \n",
    "    for page in range(n_pages):\n",
    "        # Create figures for this page\n",
    "        fig_ll, axes_ll = plt.subplots(n_rows, n_cols, figsize=(15, 10))\n",
    "        fig_dd, axes_dd = plt.subplots(n_rows, n_cols, figsize=(15, 10))\n",
    "        \n",
    "        # Flatten axes for easier indexing\n",
    "        axes_ll = axes_ll.flatten()\n",
    "        axes_dd = axes_dd.flatten()\n",
    "        \n",
    "        ll_figs.append(fig_ll)\n",
    "        ll_axes.append(axes_ll)\n",
    "        dd_figs.append(fig_dd)\n",
    "        dd_axes.append(axes_dd)\n",
    "    \n",
    "    # Process each variable\n",
    "    for i, var_info in enumerate(all_vars):\n",
    "        # Determine which page and which subplot position\n",
    "        page_idx = i // vars_per_page\n",
    "        subplot_idx = i % vars_per_page\n",
    "        \n",
    "        var_name = var_info['name']\n",
    "        \n",
    "        # Special handling for delta_z\n",
    "        if var_name == 'delta_z':\n",
    "            try:\n",
    "                ll_delta_z = np.array(ak.to_numpy(mc_ll['L0_ENDVERTEX_Z']) - ak.to_numpy(mc_ll['L0_OWNPV_Z']))\n",
    "                dd_delta_z = np.array(ak.to_numpy(mc_dd['L0_ENDVERTEX_Z']) - ak.to_numpy(mc_dd['L0_OWNPV_Z']))\n",
    "                \n",
    "                # Process delta_z \n",
    "                process_variable_single(ll_axes[page_idx][subplot_idx], ll_delta_z, var_info, \"LL\")\n",
    "                process_variable_single(dd_axes[page_idx][subplot_idx], dd_delta_z, var_info, \"DD\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing delta_z: {e}\")\n",
    "                ll_axes[page_idx][subplot_idx].text(0.5, 0.5, f\"Error processing delta_z\", \n",
    "                               ha='center', va='center', transform=ll_axes[page_idx][subplot_idx].transAxes)\n",
    "                dd_axes[page_idx][subplot_idx].text(0.5, 0.5, f\"Error processing delta_z\", \n",
    "                               ha='center', va='center', transform=dd_axes[page_idx][subplot_idx].transAxes)\n",
    "        \n",
    "        # Special handling for Bu_DTF_chi2\n",
    "        elif var_name == 'Bu_DTF_chi2':\n",
    "            try:\n",
    "                # Convert to numpy and handle the data type properly\n",
    "                ll_dtf_chi2_raw = ak.to_numpy(mc_ll[\"Bu_DTF_chi2\"])\n",
    "                dd_dtf_chi2_raw = ak.to_numpy(mc_dd[\"Bu_DTF_chi2\"])\n",
    "                \n",
    "                # Handle the data structure properly\n",
    "                ll_dtf_chi2 = []\n",
    "                dd_dtf_chi2 = []\n",
    "                \n",
    "                # Check if the arrays are already flat or if they need to be processed\n",
    "                if isinstance(ll_dtf_chi2_raw, np.ndarray) and ll_dtf_chi2_raw.ndim == 1:\n",
    "                    # If it's already a flat array, use it directly\n",
    "                    ll_dtf_chi2 = ll_dtf_chi2_raw\n",
    "                else:\n",
    "                    # Process each entry - safely extracting first element if possible\n",
    "                    for item in ll_dtf_chi2_raw:\n",
    "                        try:\n",
    "                            # Try to access first element\n",
    "                            if isinstance(item, (list, np.ndarray)) and len(item) > 0:\n",
    "                                ll_dtf_chi2.append(float(item[0]))\n",
    "                            elif hasattr(item, \"__iter__\") and len(list(item)) > 0:\n",
    "                                ll_dtf_chi2.append(float(list(item)[0]))\n",
    "                            else:\n",
    "                                # If it's a scalar, use it directly\n",
    "                                ll_dtf_chi2.append(float(item))\n",
    "                        except (IndexError, TypeError, ValueError) as e:\n",
    "                            # Skip problematic entries\n",
    "                            ll_dtf_chi2.append(np.nan)\n",
    "                            print(f\"Skipping entry in LL Bu_DTF_chi2: {e}\")\n",
    "                \n",
    "                # Same for DD\n",
    "                if isinstance(dd_dtf_chi2_raw, np.ndarray) and dd_dtf_chi2_raw.ndim == 1:\n",
    "                    dd_dtf_chi2 = dd_dtf_chi2_raw\n",
    "                else:\n",
    "                    for item in dd_dtf_chi2_raw:\n",
    "                        try:\n",
    "                            if isinstance(item, (list, np.ndarray)) and len(item) > 0:\n",
    "                                dd_dtf_chi2.append(float(item[0]))\n",
    "                            elif hasattr(item, \"__iter__\") and len(list(item)) > 0:\n",
    "                                dd_dtf_chi2.append(float(list(item)[0]))\n",
    "                            else:\n",
    "                                dd_dtf_chi2.append(float(item))\n",
    "                        except (IndexError, TypeError, ValueError) as e:\n",
    "                            dd_dtf_chi2.append(np.nan)\n",
    "                            print(f\"Skipping entry in DD Bu_DTF_chi2: {e}\")\n",
    "                \n",
    "                # Convert to numpy arrays\n",
    "                ll_dtf_chi2 = np.array(ll_dtf_chi2, dtype=float)\n",
    "                dd_dtf_chi2 = np.array(dd_dtf_chi2, dtype=float)\n",
    "                \n",
    "                # Remove NaN values\n",
    "                ll_dtf_chi2 = ll_dtf_chi2[~np.isnan(ll_dtf_chi2)]\n",
    "                dd_dtf_chi2 = dd_dtf_chi2[~np.isnan(dd_dtf_chi2)]\n",
    "                \n",
    "                # Debug info\n",
    "                print(f\"LL DTF chi2 shape: {ll_dtf_chi2.shape}, type: {type(ll_dtf_chi2)}\")\n",
    "                print(f\"DD DTF chi2 shape: {dd_dtf_chi2.shape}, type: {type(dd_dtf_chi2)}\")\n",
    "                \n",
    "                # Ensure we have valid data before plotting\n",
    "                if len(ll_dtf_chi2) > 0:\n",
    "                    process_variable_single(ll_axes[page_idx][subplot_idx], ll_dtf_chi2, var_info, \"LL\")\n",
    "                else:\n",
    "                    ll_axes[page_idx][subplot_idx].text(0.5, 0.5, \"No valid Bu_DTF_chi2 data for LL\", \n",
    "                               ha='center', va='center', transform=ll_axes[page_idx][subplot_idx].transAxes)\n",
    "                \n",
    "                if len(dd_dtf_chi2) > 0:\n",
    "                    process_variable_single(dd_axes[page_idx][subplot_idx], dd_dtf_chi2, var_info, \"DD\")\n",
    "                else:\n",
    "                    dd_axes[page_idx][subplot_idx].text(0.5, 0.5, \"No valid Bu_DTF_chi2 data for DD\", \n",
    "                               ha='center', va='center', transform=dd_axes[page_idx][subplot_idx].transAxes)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing Bu_DTF_chi2: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                ll_axes[page_idx][subplot_idx].text(0.5, 0.5, f\"Error processing Bu_DTF_chi2: {e}\", \n",
    "                               ha='center', va='center', transform=ll_axes[page_idx][subplot_idx].transAxes)\n",
    "                dd_axes[page_idx][subplot_idx].text(0.5, 0.5, f\"Error processing Bu_DTF_chi2: {e}\", \n",
    "                               ha='center', va='center', transform=dd_axes[page_idx][subplot_idx].transAxes)\n",
    "        \n",
    "        # Special handling for KK product\n",
    "        elif var_name == 'kk_product':\n",
    "            try:\n",
    "                # Get the individual kaon ID probabilities\n",
    "                ll_h1_probnnk = np.array(ak.to_numpy(mc_ll['h1_ProbNNk']))\n",
    "                ll_h2_probnnk = np.array(ak.to_numpy(mc_ll['h2_ProbNNk']))\n",
    "                dd_h1_probnnk = np.array(ak.to_numpy(mc_dd['h1_ProbNNk']))\n",
    "                dd_h2_probnnk = np.array(ak.to_numpy(mc_dd['h2_ProbNNk']))\n",
    "                \n",
    "                # Calculate the product\n",
    "                ll_kk_product = ll_h1_probnnk * ll_h2_probnnk\n",
    "                dd_kk_product = dd_h1_probnnk * dd_h2_probnnk\n",
    "                \n",
    "                # Process KK product\n",
    "                process_variable_single(ll_axes[page_idx][subplot_idx], ll_kk_product, var_info, \"LL\")\n",
    "                process_variable_single(dd_axes[page_idx][subplot_idx], dd_kk_product, var_info, \"DD\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing KK product: {e}\")\n",
    "                ll_axes[page_idx][subplot_idx].text(0.5, 0.5, f\"Error processing KK product: {e}\", \n",
    "                               ha='center', va='center', transform=ll_axes[page_idx][subplot_idx].transAxes)\n",
    "                dd_axes[page_idx][subplot_idx].text(0.5, 0.5, f\"Error processing KK product: {e}\", \n",
    "                               ha='center', va='center', transform=dd_axes[page_idx][subplot_idx].transAxes)\n",
    "        \n",
    "        # Standard variables\n",
    "        else:\n",
    "            try:\n",
    "                # Convert to numpy arrays to avoid awkward array issues\n",
    "                try:\n",
    "                    ll_data = np.array(ak.to_numpy(mc_ll[var_name]))\n",
    "                    dd_data = np.array(ak.to_numpy(mc_dd[var_name]))\n",
    "                    \n",
    "                    # Process the variable for each plot separately\n",
    "                    process_variable_single(ll_axes[page_idx][subplot_idx], ll_data, var_info, \"LL\")\n",
    "                    process_variable_single(dd_axes[page_idx][subplot_idx], dd_data, var_info, \"DD\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error extracting {var_name}: {e}\")\n",
    "                    ll_axes[page_idx][subplot_idx].text(0.5, 0.5, f\"Error extracting {var_name}\", \n",
    "                               ha='center', va='center', transform=ll_axes[page_idx][subplot_idx].transAxes)\n",
    "                    dd_axes[page_idx][subplot_idx].text(0.5, 0.5, f\"Error extracting {var_name}\", \n",
    "                               ha='center', va='center', transform=dd_axes[page_idx][subplot_idx].transAxes)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing variable {var_name}: {e}\")\n",
    "                ll_axes[page_idx][subplot_idx].text(0.5, 0.5, f\"Error in processing\", \n",
    "                           ha='center', va='center', transform=ll_axes[page_idx][subplot_idx].transAxes)\n",
    "                dd_axes[page_idx][subplot_idx].text(0.5, 0.5, f\"Error in processing\", \n",
    "                           ha='center', va='center', transform=dd_axes[page_idx][subplot_idx].transAxes)\n",
    "    \n",
    "    # Handle any unused subplots in the last page\n",
    "    remaining = vars_per_page - (n_vars % vars_per_page)\n",
    "    if remaining < vars_per_page:  # Only if it's not a full page\n",
    "        last_page = n_pages - 1\n",
    "        for j in range(vars_per_page - remaining, vars_per_page):\n",
    "            if j < len(ll_axes[last_page]):\n",
    "                ll_axes[last_page][j].set_visible(False)\n",
    "                dd_axes[last_page][j].set_visible(False)\n",
    "    \n",
    "    # Save all figures with appropriate titles and layout adjustments\n",
    "    for page in range(n_pages):\n",
    "        # Add titles to each page\n",
    "        ll_figs[page].suptitle(f\"Selection Variables - LL Sample (Page {page+1}/{n_pages})\", fontsize=16)\n",
    "        dd_figs[page].suptitle(f\"Selection Variables - DD Sample (Page {page+1}/{n_pages})\", fontsize=16)\n",
    "        \n",
    "        # Adjust layout with more space\n",
    "        plt.figure(ll_figs[page].number)\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.95])  # Add padding for the title\n",
    "        \n",
    "        plt.figure(dd_figs[page].number)\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.95])  # Add padding for the title\n",
    "        \n",
    "        # Generate filenames with page numbers\n",
    "        ll_filename = f\"{output_prefix}_LL_data_page{page+1}.pdf\"\n",
    "        dd_filename = f\"{output_prefix}_DD_data_page{page+1}.pdf\"\n",
    "        \n",
    "        plt.figure(ll_figs[page].number)\n",
    "        plt.savefig(ll_filename, dpi=300, bbox_inches='tight')\n",
    "        \n",
    "        plt.figure(dd_figs[page].number)\n",
    "        plt.savefig(dd_filename, dpi=300, bbox_inches='tight')\n",
    "        \n",
    "        print(f\"LL plot page {page+1} saved to {ll_filename}\")\n",
    "        print(f\"DD plot page {page+1} saved to {dd_filename}\")\n",
    "        \n",
    "        plt.close(ll_figs[page])\n",
    "        plt.close(dd_figs[page])\n",
    "    \n",
    "    return\n",
    "\n",
    "def process_variable_single(ax, data, var_info, sample_type):\n",
    "    \"\"\"\n",
    "    Process and plot a single variable for a single sample\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    ax : matplotlib.axes.Axes\n",
    "        Axes to plot on\n",
    "    data : numpy.ndarray\n",
    "        Data from sample\n",
    "    var_info : dict\n",
    "        Variable information (name, label, cut_value, cut_type)\n",
    "    sample_type : str\n",
    "        Sample type (\"LL\" or \"DD\")\n",
    "    \"\"\"\n",
    "    var_name = var_info['name']\n",
    "    var_label = var_info['label']\n",
    "    cut_value = var_info['cut_value']\n",
    "    cut_type = var_info['cut_type']\n",
    "    \n",
    "    # Make sure data is a numpy array\n",
    "    data = np.array(data, dtype=float)\n",
    "    \n",
    "    # Filter out any NaN or inf values\n",
    "    data = data[~np.isnan(data) & ~np.isinf(data)]\n",
    "    \n",
    "    # Skip if no data left\n",
    "    if len(data) == 0:\n",
    "        ax.text(0.5, 0.5, \"No valid data available\", \n",
    "                ha='center', va='center', transform=ax.transAxes)\n",
    "        return\n",
    "    \n",
    "    # Calculate pass percentage\n",
    "    if cut_type == '>':\n",
    "        pass_percent = (data > cut_value).sum() / len(data) * 100\n",
    "    else:  # '<'\n",
    "        pass_percent = (data < cut_value).sum() / len(data) * 100\n",
    "    \n",
    "    # Create bins for histogram\n",
    "    min_val = np.min(data)\n",
    "    max_val = np.max(data)\n",
    "    \n",
    "    # Special handling for some variables with extreme ranges\n",
    "    if max_val - min_val > 1000 and 'PT' in var_name:\n",
    "        # For PT variables, focus on the important range\n",
    "        min_val = max(0, min_val)\n",
    "        max_val = min(10000, max_val)\n",
    "    elif max_val - min_val > 1000 and 'CHI2' in var_name:\n",
    "        # For CHI2 variables, focus on the important range\n",
    "        min_val = max(0, min_val)\n",
    "        max_val = min(500, max_val)\n",
    "    \n",
    "    # Add padding to range\n",
    "    range_padding = (max_val - min_val) * 0.1\n",
    "    hist_range = (min_val - range_padding, max_val + range_padding)\n",
    "    \n",
    "    # Set number of bins based on data range\n",
    "    if max_val - min_val > 100:\n",
    "        bins = 50\n",
    "    else:\n",
    "        bins = 30\n",
    "    \n",
    "    # Create histogram with raw counts (not density)\n",
    "    hist, bins = np.histogram(data, bins=bins, range=hist_range, density=False)\n",
    "    \n",
    "    # Calculate bin centers\n",
    "    centers = (bins[:-1] + bins[1:]) / 2\n",
    "    \n",
    "    # Plot histograms as step plots (cleaner than bars)\n",
    "    color = 'blue' if sample_type == 'LL' else 'green'\n",
    "    ax.step(centers, hist, where='mid', color=color, linewidth=2)\n",
    "    \n",
    "    # Add vertical line at cut value\n",
    "    ymin, ymax = 0, np.max(hist) * 1.1\n",
    "    ax.set_ylim(ymin, ymax)\n",
    "    ax.vlines(cut_value, ymin, ymax, colors='red', linestyles='dashed', linewidth=2)\n",
    "    \n",
    "    # Add cut text at the top of the graph\n",
    "    ax.text(\n",
    "        0.5, \n",
    "        0.95, \n",
    "        f\"Cut: {cut_type} {cut_value} ({pass_percent:.1f}% pass)\", \n",
    "        transform=ax.transAxes, \n",
    "        verticalalignment='top', \n",
    "        horizontalalignment='center',\n",
    "        bbox=dict(boxstyle='round', facecolor='white', alpha=0.8)\n",
    "    )\n",
    "    \n",
    "    # Set title and labels\n",
    "    ax.set_title(f\"{var_label} - {sample_type}\")\n",
    "    ax.set_xlabel(var_label)\n",
    "    ax.set_ylabel('Events')\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LL DTF chi2 shape: (713632,), type: <class 'numpy.ndarray'>\n",
      "DD DTF chi2 shape: (561928,), type: <class 'numpy.ndarray'>\n",
      "LL plot page 1 saved to selection_variables_LL_data_page1.pdf\n",
      "DD plot page 1 saved to selection_variables_DD_data_page1.pdf\n",
      "LL plot page 2 saved to selection_variables_LL_data_page2.pdf\n",
      "DD plot page 2 saved to selection_variables_DD_data_page2.pdf\n"
     ]
    }
   ],
   "source": [
    "plot_selection_variables_separate(data_ll, data_dd, output_prefix=\"selection_variables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
